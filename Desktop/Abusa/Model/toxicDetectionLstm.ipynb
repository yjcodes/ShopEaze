{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model:\n",
    "\n",
    "We will  be taking a LSTM model for our multi label predictions. We are taking this deep learning model since it can give a good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import codecs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0      0             0        0       0       0              0   \n",
       "1      0             0        0       0       0              0   \n",
       "2      0             0        0       0       0              0   \n",
       "3      0             0        0       0       0              0   \n",
       "4      0             0        0       0       0              0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  explanation edits made username hardcore metal...  \n",
       "1  daww matches background colour seemingly stuck...  \n",
       "2  hey man really trying edit war guy constantly ...  \n",
       "3  make real suggestions improvement wondered sec...  \n",
       "4                      sir hero chance remember page  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('preprocessed_text.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hi back last warning stop undoing edits die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>think fagget get oife burn hell hate sorry can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>also sock puppet account suprise sincerely man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>fuck smith please notified die want dance grave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wouldnt first time bitch fuck ill find live so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "79        1             0        0       1       0              0   \n",
       "176       1             0        1       1       1              1   \n",
       "600       1             0        0       1       0              0   \n",
       "802       1             0        1       1       1              0   \n",
       "1017      1             1        1       1       1              1   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "79          hi back last warning stop undoing edits die  \n",
       "176   think fagget get oife burn hell hate sorry can...  \n",
       "600   also sock puppet account suprise sincerely man...  \n",
       "802     fuck smith please notified die want dance grave  \n",
       "1017  wouldnt first time bitch fuck ill find live so...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['threat']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate',\n",
      "       'preprocessed_text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23228</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>aids account also closed due pool aids sorry v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11448</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>really even obvious fine hope know head heart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12986</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>report want call vandalism idiocy begin whethe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37182</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nomeansno move hello disagree greatly decision...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120292</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>december according webpage jay hi name jay kno...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "23228       1             0        0       0       0              0   \n",
       "11448       0             0        0       0       0              0   \n",
       "12986       1             0        0       0       1              0   \n",
       "37182       0             0        0       0       0              0   \n",
       "120292      0             0        0       0       0              0   \n",
       "\n",
       "                                        preprocessed_text  \n",
       "23228   aids account also closed due pool aids sorry v...  \n",
       "11448   really even obvious fine hope know head heart ...  \n",
       "12986   report want call vandalism idiocy begin whethe...  \n",
       "37182   nomeansno move hello disagree greatly decision...  \n",
       "120292  december according webpage jay hi name jay kno...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train,test=train_test_split(df,test_size=0.2)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of words for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 400\n",
    "MAX_NB_WORDS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data tensor: (127567, 400)\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer(lower=False, filters='',num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train['preprocessed_text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train['preprocessed_text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['preprocessed_text'])\n",
    "\n",
    "train_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "nb_words = (np.max(train_data) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(nb_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 400, 128)          6400000   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 400, 60)           45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 6,448,716\n",
      "Trainable params: 6,448,716\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "inp = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "# size of the vector space\n",
    "embed_size = 128\n",
    "x = Embedding(nb_words, embed_size)(inp)\n",
    "output_dimention = 60\n",
    "x = LSTM(output_dimention, return_sequences=True,name='lstm_layer')(x)\n",
    "# reduce dimention\n",
    "x = GlobalMaxPool1D()(x)\n",
    "# disable 10% precent of the nodes\n",
    "x = Dropout(0.1)(x)\n",
    "# pass output through a RELU function\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "# another 10% dropout\n",
    "x = Dropout(0.1)(x)\n",
    "# pass the output through a sigmoid layer, since \n",
    "# we are looking for a binary (0,1) classification \n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114810 samples, validate on 12757 samples\n",
      "Epoch 1/2\n",
      "114810/114810 [==============================] - 1118s 10ms/step - loss: 0.0739 - accuracy: 0.9763 - val_loss: 0.0515 - val_accuracy: 0.9806\n",
      "Epoch 2/2\n",
      "114810/114810 [==============================] - 1203s 10ms/step - loss: 0.0436 - accuracy: 0.9834 - val_loss: 0.0484 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1f8addb4198>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['identity_hate', 'insult', 'obscene', 'severe_toxic', 'threat', 'toxic']\n",
    "y = train[labels].values\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "model.fit(train_data,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0373957e-04, 3.3771992e-04, 1.6841292e-04, 6.0232010e-06,\n",
       "        2.0135340e-05, 1.9673705e-03],\n",
       "       [5.4430373e-05, 1.9407272e-04, 1.1147541e-04, 4.3026052e-06,\n",
       "        1.2279527e-05, 1.2706220e-03],\n",
       "       [6.6503460e-05, 6.5809488e-04, 2.2116303e-04, 1.6159886e-06,\n",
       "        7.6981214e-06, 4.7862530e-03],\n",
       "       [1.3959408e-04, 4.2742491e-04, 2.4601817e-04, 1.1346216e-05,\n",
       "        3.8469028e-05, 2.6203096e-03],\n",
       "       [1.5312433e-04, 4.5421720e-04, 2.6294589e-04, 1.2034592e-05,\n",
       "        3.6830697e-05, 2.6741624e-03],\n",
       "       [8.2526312e-05, 9.4771385e-04, 3.0827522e-04, 1.5139155e-06,\n",
       "        7.6391834e-06, 6.9566667e-03],\n",
       "       [5.5186510e-02, 1.1440787e-01, 3.7994921e-02, 1.8342137e-03,\n",
       "        1.0418594e-02, 3.5571337e-01],\n",
       "       [6.6113472e-04, 3.0002892e-03, 1.4289320e-03, 5.2680261e-05,\n",
       "        2.8517842e-04, 1.9589305e-02],\n",
       "       [1.4137788e-05, 9.2093644e-05, 3.7664293e-05, 3.4146836e-07,\n",
       "        8.7306250e-07, 6.2993169e-04],\n",
       "       [2.7304499e-05, 1.3545156e-04, 5.3559837e-05, 6.8095932e-07,\n",
       "        2.0479310e-06, 8.6241961e-04]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(test_data)\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('LSTM_toxic_prediction_model.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We are storing our model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''##########[!] Functions reused in Django###############'''\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "def clear_sentance(sentance):\n",
    "    sentance= re.sub(r\"http\\S+\", \"\", sentance)\n",
    "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n",
    "    sentance = decontracted(sentance)\n",
    "    sentance = cleanPunc(sentance)\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    #sentance = stemming(sentance)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    #https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in  stopwords.words('english'))\n",
    "    return sentance.strip()\n",
    "def tokenize(sentance):\n",
    "    MAX_SEQUENCE_LENGTH = 400\n",
    "    #MAX_NB_WORDS = 50000\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "                    tokenizer = pickle.load(handle)\n",
    "    test_sequences = tokenizer.texts_to_sequences([sentance])\n",
    "    test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return test_data\n",
    "def model_predict(test_data):\n",
    "    from keras.models import load_model\n",
    "    model=load_model('LSTM_toxic_prediction_model.h5')\n",
    "    prediction=model.predict(test_data)\n",
    "    \n",
    "    return prediction\n",
    "def get_prediction(sentance):\n",
    "    clear_text=clear_sentance(sentance)\n",
    "    test_data=tokenize(clear_text)\n",
    "    predicted_array=model_predict(test_data)\n",
    "    #'identity_hate', 'insult', 'obscene', 'severe_toxic', 'threat', 'toxic'\n",
    "    predicted_values={'Hate':round(predicted_array[0][0]),'Insult':round(predicted_array[0][1]), 'Obscene':round(predicted_array[0][2]), 'Severe Toxic':round(predicted_array[0][3]), 'Threat':round(predicted_array[0][4]), 'Toxic':round(predicted_array[0][5])}\n",
    "    #print(clear_text)\n",
    "    #print(test_data)\n",
    "    #print(predicted_array)\n",
    "    result=''\n",
    "    for key in predicted_values:\n",
    "        #print(key)\n",
    "        #print(predicted_values[key])\n",
    "        if(predicted_values[key]==1.0):\n",
    "            result+=key+' '\n",
    "    print(result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate Insult Toxic \n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"pair jew hating weiner nazi schmucks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insult Obscene Toxic \n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Fuck OFF man , you peace of cunt. Mother fucker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insult Obscene Toxic \n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"BAstard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
